---
title: "Geography AI Workshop 02 Convolutional neural networks"
knit: (function(input_file, encoding) {
  out_dir <- 'docs';
  rmarkdown::render(input_file,
 encoding=encoding,
 output_file=file.path(dirname(input_file), out_dir, 'ESS_02_CNNS.html'))})
author: | 
  | Simon Brewer
  | Geography Department
  | University of Utah
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  html_document:
    toc: true
    toc_float: true
    fig_caption: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(png)
library(grid)
```

```{r echo=FALSE}
set.seed(1234)
```

## Introduction

In this exercise, we'll build a convolutional neural network (CNN) for image classification. This is one of the original and more straightforward uses of CNNs. More complex uses include:

- Semantic image segmentation
- Image denoising or reconstruction
- Working with video streams

But all methods are based around two basic operations: 

- Convolution: in this step, the network learns a series of kernels or filters that transform the original image in some way. These are similar to filters that are used in standard image processing (e.g. low-pass filters), but filters are chosen by how well the transformed image maps to the outcome variable. To put this another way, these filters identify shapes or features that are important in differentiating between different outcomes
- Max-pooling: in this step, the image resolution is transformed. In general, the resolution is halved, by aggregating groups of four pixels in a two by two window. Pooling acts to aggregate smaller scale features into larger objects

In general, these steps are repeated several times. As this progresses, the small shapes identified in the first set of convolutions are progressively combined into larger structures. For example, a series of small curves or lines could be aggregated into a cat's eye.

The code in this example is modified from a blog by BEEILAB: 
https://medium.com/@beeilab.yt/land-use-land-cover-classification-using-satellite-images-and-deep-learning-a-step-by-step-guide-27fea9dbf748

## Tensorflow and Keras in R

The TensorFlow and Keras libraries were first designed to be used in Python. There has been quite a lot of work recently to allow access in R, mainly using the `reticulate` package which allows exchanges between R and Python, but the installation can still be tricky. Please ask if you need help with any of these steps. (If this works, you will only need to do this once!)

First, you need to install a version of Python. I'd recommend installing [Anaconda][anaconda] or [miniconda][miniconda], as these generally work well. You can also follow this guide: 
https://medium.datadriveninvestor.com/how-to-setup-keras-and-tensorflow-in-rstudio-on-windows-d0175e9a8f10

Next, open R/RStudio and install the R `keras` package:

```{r eval = FALSE}
install.packages('keras')
```

Once this step is complete, load the library, and run the `install_keras()` function. This will download all the necessary Python add-ons to build deep learning models:

```{r eval = FALSE}
library(keras)
install_keras()
```

This can take a few minutes, but should all load without problem. If you do get error messages, please let us know so we can try to fix them. If this all works, move on to the next section.

## Image classification

The basic idea behind image classification is to link *features* of an image to a single label or class. For example, we might have a photograph of a cat, with the label `Cat` and one of a dog with the label `Dog`. The goal of the model is to identify what shapes and colors, and combination of these can help differentiate between these two classes. 

### Data

For this exercise, we'll use the EuroSat benchmarking dataset from: https://zenodo.org/records/7711810:

> EuroSAT is a land use and land cover classification dataset. The dataset is based on Sentinel-2 satellite imagery covering 13 spectral bands and consists of 10 LULC classes with a total of 27,000 labeled and geo-referenced images. 

The images are available as both multispectral and RGB. We'll just use the RGB images here to limit the amount of data we need to deal with, but this code can be easily extended to use the multispectral data. In the paper that accompanies this dataset, the authors were able to get a 98.5% accuracy in prediction. We won't get that high here, as we'll take a few short cuts to make the code run faster, but this should give you an idea of what is possible with these networks. 

The images are available on the workshop Google drive in the zip file *EuroSat_RGB.zip*. Download this now, and move it to a folder that is easy to find on your computer, and unzip it. 

Once you have unzipped the data, take a look in the *EuroSat_RGB* folder. This is already set up in the standard way for image classification, where all images for a given class are kept in a single folder for that class. The name of the folders can then used as the *label* for each image, and is what we will use. 

- EuroSat_RGB
- AnnualCrop
- Forest
- HerbaceousVegetation
- ...

Note that this is a specific layout for image classification. There are other options, for example, if you are working with continuous outcomes.

### Data processing

Let's start, as usual, by loading the libraries we'll need for the lab:

```{r message=FALSE}
library(dplyr)
library(tidyr)
library(terra)
library(keras)
library(ggplot2)
```

Now set the path to the folder containing the training images you downloaded. If you've copied these to your datafiles folder, this will look something like this:

```{r eval=TRUE}
image_path = "./datafiles/EuroSAT_RGB/"
```

If you have any questions about setting this path, please ask. 

You can visualize any of the images using the **imager** package (you'll need to install this):

```{r message = FALSE}
library(imager)
im = load.image(paste0(image_path, "Forest/Forest_10.jpg"))
plot(im)
```

You can try other images by changing the folder name and filename. Note that these are somewhat idealized images, with a blank white background. 

Next, we'll define the classes that we are going to process. There are 16 different types of fruit in the dataset, making this a multi-class classification problem. 

```{r}
class_list = c("AnnualCrop", "Forest", "HerbaceousVegetation",
               "Highway", "Industrial", "Pasture", "PermanentCrop",
               "Residential", "River", "SeaLake")

# store the number of classes
num_classes = length(class_list)
```

The original images are 64x64 pixels. As these are small, we can use them at their full resolution. For larger images, we'd either need to reduce their resolution or divide them into smaller images. 

We'll set a few constants here: the image size (`patch_size`) and the number of images we want to use from each class (`patch_number`). The full set has 3000 images per class, but here will limit this to the first 1000 to speed up model training. A good follow-up exercise would be to increase this to see what impact it has on the model's predictive skill. We also define the number of channels - these will define the input tensors. These will be rank 3 tensors, with dimensions 64x64x3. The channels can easily be changed here to include additional bands as necessary (Sentinel-2 multispectral data has 13 bands, for example).

```{r}
# Specify the number of images to use
patch_numbers = 1000

# Define the size of each image patch
patch_size_x = 64
patch_size_y = 64

# Specify the number of spectral bands in your satellite imagery
channels = 3
```

Now we'll create some empty arrays to store the information of both the images (`X`) and labels or classes (`y`). As this is a (relatively) small dataset, we can load the images into memory. For larger sets of images, the tensorflow Keras API provides a series of data loaders that can pass images in batches from disk and avoid memory issues.

```{r}
X = array(0, dim = c(patch_numbers * num_classes, 
                     patch_size_x, patch_size_y, channels)) ## Images
Y = array(0, dim = c(patch_numbers * num_classes)) ## Labels
```

Now we'll read in and store the images in a big loop. We'll first iterate over the different class folders, then over the images contained within them. We use `rast` from the **terra** package to read in the image, then copy the image into the `X` array created earlier. At the same time, we store the folder name in `y` as the label. As these are 8-bit RGB images, the intensity values range from 0-255. We standardize to a [0-1] range by dividing the pixel values by 255. This form of standardization (min-max standardization) is frequently used with neural networks to avoid issues in weight calculation.

```{r warning=FALSE}
library(terra)
counter = 1
for (i in 1:length(class_list)) {
  print(i)
  # print(paste0(image_path, class_list[i]))
  image_list = list.files(paste0(image_path, class_list[i]))
  image_list = image_list[0:patch_numbers]
  for (j in image_list) {
    image_file = paste0(image_path, class_list[i], "/", j)
    image = rast(image_file)
    
    X[counter, , , ] = as.array(image) / 255
    Y[counter] <- i - 1
    counter = counter + 1
  }
}

```

### Training and test sets

Before building the CNN model, we'll subdivide our data into three subsets: training, testing and validation. We'll use R's `sample()` function to generate a random set of rows to extract, and we'll do it in two steps. First we use a 80/20 split to remove 20% of the original data for testing. This will be held in reserve during model training and only used to assess the final model:

```{r}
train_id <- sample(1:nrow(X), nrow(X)  * 0.8)
X_train = X[train_id, , , ]
y_train = Y[train_id]
X_test = X[-train_id, , , ]
y_test = Y[-train_id]
```

Second, we split the other 80% into a training and validation set using another 80/20 split. The training set will be used to set model weights, and the validation set used to assess performance during training. 

```{r}
train_id <- sample(1:nrow(X_train), nrow(X_train)  * 0.8)
X_val = X_train[-train_id, , , ]
y_val = y_train[-train_id]
X_train = X_train[train_id, , , ]
y_train = y_train[train_id]
```

```{r}
y_train <- to_categorical(y_train)
y_val <- to_categorical(y_val)
y_test <- to_categorical(y_test)
```

Before moving on, we'll define some variables to represent the image width, height and number of channels. These will be used to represent the input tensor dimensions for the model

```{r}
img_width = dim(X_train[1, , , ])[1]
img_height = dim(X_train[1, , , ])[2]
channels = dim(X_train[1, , , ])[3]
```

## CNN model

Let's now set up the model. We'll now set up the model. There are a few things we need to define here:
- The loss function
- Performance metrics
- Model optimizer
- The architecture

### Loss function

The loss function is used during training to measure how well the current set of model weights map between inputs and output, and is then used in adjusting these. A good option for the loss function is `categorical_crossentropy`, which tries to maximize the accuracy across multiple groups

```{r}
my_loss = "categorical_crossentropy"
```

### Performance metric

The metric is the error between the predicted class and observed class aggregated across all samples. There are a large number of these, but here we'll us accuracy. This is probably the simplest of all metrics and gives the proportion of all images that are correctly classified.

```{r}
my_metrics = 'accuracy'
```

### Optimizer

The optimizer is used to adjust weights during the backpropagation or training of the network. We'll use the `Adam` optimizer - this is well suited to large and complex problems, and is computationally very efficient. We'll set the learning rate to 0.0001. This is an important hyperparameter in these models as it this limits the changes to model weights during training, and can help limit overfitting. If you have time, it's worth re-training the model with a higher or lower rate to see the effect. 

```{r}
my_opt = optimizer_rmsprop(learning_rate = 0.00001)
```

### Model architecture

Before building the full model, we'll make a simple CNN with a single convolution/max-pooling step to illustrate how the code and snytax work

First, create a template sequential model

```{r}
model = keras_model_sequential() 
```

Next we add the first hidden layer. This is a convolutional layer, where we'll create 32 filters (or convolutions) based on the original images, with a 3x3 kernel. We'll pad the output of this layer so that it has the same size as the input (`same`), and give this a ReLU activation function to transform the weights. Note that as this is the first layer, we also need to define the size of the input tensors (width, height and channels). 

```{r}
model %>%
  layer_conv_2d(filter = 32, kernel_size = c(3,3), padding = "same", 
                input_shape = c(64, 64, 3),
                activation = "relu")
```

If you print the model object now, you'll see that it has been updated.

```{r}
model
```
Now, we add a max-pooling layer. As a reminder, this reduces the resolution of the output from the previous layer by a simple filter, forcing the next layer of the network to focus on larger image features. 

```{r}
model %>%
  layer_max_pooling_2d(pool_size = c(2,2))
```

Now we'll add layers to connect the output of this max-pooling step to the output (the land cover classes). The first thing we need to do is to flatten the output. The output of the max-pooling is a tensor of shape (32, 32, 32). The reduction in the height and width is a result of the max-pooling operation and the 32 is the number of filters from the convolution. The `layer_flatten()` function will flatten this into a rank 1 tensor of shape 32768 (32^3). 

```{r}
model %>%
  layer_flatten()
```

Next we'll pass this flattened layer through a dense layer, with a ReLU activation. This will provides one more set of weights before we connect to the output.

```{r}
model %>% 
  layer_dense(128, activation = "relu")
```

Finally, we need a layer to represent the predictions. As this is a multiclass task, the final layer needs to have the same number of nodes as classes (10). This is passed through a softmax activation function. This transforms the predictions for all classes into probabilities (i.e. they have to sum to 1). 

```{r}
model %>%
  layer_dense(num_classes, activation = "softmax")
```

More practically, we'll create a function that will build the model in one go, which allows us to to easily create new versions for testing. We'll create a slightly more complex model, with three successive convolution/max-pooling steps, and two dense layers before the output. We'll also add batch normalization layers; these take the output of the previous layer and normalize the weights. This is a simple method that adjusts the mean weight to close to zero and reduces the amount of variation. This helps avoid gradient problems with very small or very large weights


```{r}
create_model = function(img_width, img_height, 
                        channels, num_classes=10) {
  model <- keras_model_sequential() %>%
    
    # First layer: 1 convolution, 1 pooling
    layer_conv_2d(filter = 32, kernel_size = c(3,3), 
                  input_shape = c(img_width, img_height, channels),
                  activation = "relu") %>%
    layer_max_pooling_2d(pool_size = c(2,2)) %>%
    layer_batch_normalization() %>%

        # Second hidden layer, repeat
    layer_conv_2d(filter = 64, kernel_size = c(3,3), 
                  activation = "relu") %>%
    layer_max_pooling_2d(pool_size = c(2,2)) %>%
    layer_batch_normalization() %>%

    # Second hidden layer, repeat
    layer_conv_2d(filter = 128, kernel_size = c(3,3), 
                  activation = "relu") %>%
    layer_max_pooling_2d(pool_size = c(2,2)) %>%
    layer_batch_normalization() %>%

    # Flatten and feed into dense layer
    layer_flatten() %>%
    layer_dense(128, activation = "relu") %>%
    layer_dense(256, activation = "relu") %>%
    
    # Outputs 
    layer_dense(num_classes, activation = "softmax")
  
  return(model)
}
```

Now let's build the model. We first set variables to image height, weight and channels (to be used in the first layer), then call the create_model function

```{r}
model = create_model(img_width, img_height, channels,
                     num_classes=num_classes)

summary(model)
```

Our model has a little under 1.2 million parameters or weights to train (hence the need for a lot of images). 

The next step is to compile the model. 

```{r}
model %>% compile(
  loss = my_loss,
  optimizer = my_opt,
  metrics = my_metrics
)
```

## Model training

We'll now train the model for 30 epochs, using the `fit()` method. We need to provide the following arguments:

- `x`: The input training data
- `y`: The training labels or classes
- `validation_data`: the generator of the validation samples
- `epochs`: number of full training iterations
- `batch_size`: number of images that are passed to the network before updating weights
- We also tell the model to `shuffle` the input images during training. This can be important as deep learning networks learn best when there is a lot of variation in the inputs, including the order that they are received

This takes a couple of minutes to train (on my laptop). It's worth remembering what is going on here: the algorithm is reading in batches of 32 images, rescaling them, updating model weights through back propagation and then repeating the whole thing 30 times. As we previously defined a separate validation set (and image generator), this routine will calculate two losses:

- The training loss. This is how accurately the model can predict the images that are being used to update the weights
- The validation loss. This is how accurately the model can predict a set of training images that are not used in updating the weights

As the model continues to train, you should see the loss (the crossentropy) decrease for both of these, but will likely stabilize at a certain point. The accuracy should (hopefully) increase over time. 

```{r eval=FALSE}
history <- model %>% fit(
  x = X_train, 
  y = y_train,
  validation_data = list(X_val, y_val),
  epochs = 20,
  batch_size = 32,
  shuffle = TRUE
)
```

```{r echo=FALSE, warning=FALSE}
history <- model %>% fit(
  x = X_train, 
  y = y_train,
  validation_data = list(X_val, y_val),
  epochs = 20,
  batch_size = 32,
  shuffle = TRUE
)
```

## Model evaluation

We'll now evaluate the trained model. First, we'll plot out the changes in the accuracy and loss function across the training epochs. This visualization is an important step as it helps show if the model has been well trained. Ideally, the loss function will show a fairly gradual decline to a minimum, and the accuracy will show a gradual increase to a plateau. If these do not stabilize, this indicates under-training and may require a higher learning rate or more epochs. If the decline in the loss is very sharp, the model may be overfit to the data. 

All the information about the model training is held in the output `history` object. 

```{r}
plot(history)
```

The plot shows a steep decline in the loss values, with a corresponding increase in accuracy. (It is possible that the model has not completely reached an optimum over the 20 epochs, but we can still work with this). Next, we'll see how the model performs in predicting classes for the test data set. As a reminder, this dataset was not used in the training (the model hasn't 'ssen' these images), and is considered to be a good independent test of predictive skill. Here, we use the `evaluate` method to calculate a testing loss and accuracy:

```{r}
results <- model %>%
  evaluate(X_test, y_test)
print(results)
```

Which gives us an accuracy of about `r round(results[2], 2)`. This is a reasonable classifier, but could be improved on. While this gives us an overall metric for the models predictive skill, we can also dig into this a little further, including looking at how well individual classes were predicted. To do this, we first predict for each test image:

```{r}
y_pred <- model %>% 
  predict(X_test) 
```

For each image, there is the predicted probability of each class:

```{r}
y_pred[1,]
```

To get the predicted labels, we simply need to find the column with the highest probability. We can use R's `max.col()` function for this (and to get the observed classes):

```{r}
prediction <- factor(max.col(y_pred) )
actual <- factor(max.col(y_test))
```

With the predicted and observed labels, we can now use **ModelMetrics** broad range of evaluation metrics. For example, to calculate the precision and recall:
- Precision is the ratio of the number of images that were correctly predicted for a class to the total number of images that were predicted for that class (correctly or incorrectly). 
- Recall is the ratio of of the number of images that were correctly predicted for a class to the total number of images in that class. This is also called the sensitivity

```{r}
library(ModelMetrics)
precision(actual, y_pred)
recall(actual, y_pred)
```

We can now make a confusion matrix between the observed and predicted classes:

```{r}
pred_table <- table(actual, prediction)
```

Note that the row and column indices are different (this is because R starts indices at 1 and Keras starts at 0). We can simply replace these with the fruit labels

```{r results='hide'}
rownames(pred_table) <- colnames(pred_table) <- class_list
pred_table
```

```{r echo=FALSE}
knitr::kable(pred_table)
```

This shows why we got such a high accuracy - nearly all the images are correctly classified. Let's finish by plotting this result. Here, we convert this confusion matrix into a long data frame, and calculate the number of each observed image class. We then use this to calculate the percentage correctly identified, and use **ggplot2**'s `geom_tile` to plot this out. Note that you will need the **reshape2** package here:

```{r}
library(reshape2)

pred_df <- melt(pred_table, value.name = "count") %>%
  group_by(actual) %>%
  mutate(n = sum(count)) %>%
  ungroup()
```

```{r}
p <- pred_df %>%
  filter(count > 0) %>%
  mutate(percentage_pred = count / n * 100) %>%
  ggplot(aes(x = actual, y = prediction, 
             fill = percentage_pred,
             label = round(percentage_pred, 2))) +
  geom_tile() +
  #scale_fill_continuous() +
  scale_fill_gradient(low = "blue", high = "red") +
  geom_text(color = "white") +
  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1)) +
  labs(x = "True class", 
       y = "Predicted class",
       fill = "Percentage\nof predictions",
       title = "True v. predicted class labels", 
       subtitle = "Percentage of test images predicted for each label")

print(p)
```

## Final thoughts

The confusion matrix shows that the class-based accuracy varies substantially (compare highways to sea/lake). Overall the predictive skill is reasonable, particularly given some of the short-cuts that were taken here (subsets of data, realtively few training epochs). There are a variety of ways that this model can be improved on:

- Using all the images
- Using the multispectral images
- Using a more complex CNN with a larger number of layers
- Using a pre-trained model (one trained on a very large number of images that can be modified for this set


[anaconda]: https://www.anaconda.com/download
[miniconda]: https://conda.io/projects/conda/en/latest/user-guide/install/windows.html